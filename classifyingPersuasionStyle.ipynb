{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import json"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "client_id = 'client id code'\n",
    "client_secret = 'client secret'\n",
    "user_agent = 'testPersuation'\n",
    "username = 'dagifft219'\n",
    "password = 'password'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "reddit = praw.Reddit(client_id=client_id,client_secret=client_secret,\n",
    "                    user_agent=user_agent,username=username,password=password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reddit_object(json_file='reddit_config.json',json_key='reddit_user_values'):\n",
    "    with open(json_file) as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    user_values = data[json_key]\n",
    "    \n",
    "    reddit = praw.Reddit(client_id=user_values['client_id'],\n",
    "                         client_secret=user_values['client_secret'],\n",
    "                         user_agent=user_values['user_agent'],\n",
    "                         username=user_values['username'],\n",
    "                         password=user_values['password'])\n",
    "    \n",
    "    return reddit\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subreddit_data(subreddit, sort_type, time='all', limit=10):\n",
    "#     top = subreddit.top(\"all\",limit=100)\n",
    "    attr = getattr(subreddit, sort_type)\n",
    "    if sort_type=='top':\n",
    "        data = attr(time, limit=limit)\n",
    "    else:\n",
    "        data = attr(limit=limit)\n",
    "        \n",
    "    top_df = pd.DataFrame()\n",
    "    for submission in data:\n",
    "        if submission.stickied:\n",
    "            pass\n",
    "        else:\n",
    "            submission.comments.replace_more(limit=None)\n",
    "            row = {}\n",
    "            row['id']= submission.id\n",
    "            row['title']= submission.title\n",
    "            row['url']= submission.url\n",
    "            row['author']= ['' if submission.author is None else submission.author.name][0]\n",
    "            row['created']= pd.to_datetime(str(submission.created),unit='s')\n",
    "            row['submission_text']= submission.selftext\n",
    "            row['all_awardings']= submission.all_awardings\n",
    "            row['category']= submission.category\n",
    "            row['discussion_type']= submission.discussion_type\n",
    "            row['distinguished']= submission.distinguished\n",
    "            row['downs']= submission.downs\n",
    "            row['gilded']= submission.gilded\n",
    "            row['gildings']= submission.gildings\n",
    "            row['likes']= submission.likes\n",
    "            row['num_comments']= submission.num_comments\n",
    "            row['num_crossposts']= submission.num_crossposts\n",
    "            row['num_reports']= submission.num_reports\n",
    "            row['score']= submission.score\n",
    "            row['top_awarded_type']= submission.top_awarded_type\n",
    "            row['total_awards_received']= submission.total_awards_received\n",
    "            row['ups']= submission.ups\n",
    "            row['upvote_ratio']= submission.upvote_ratio\n",
    "            row['view_count']= submission.view_count\n",
    "\n",
    "            top_levels = submission.comments[:]\n",
    "            comments = [i.body for i in top_levels]\n",
    "            row['comments']= comments\n",
    "\n",
    "            comment_authors = [i.author for i in top_levels]\n",
    "            comment_authors = ['' if comment_authors[i] is None else comment_authors[i].name for i in range(len(comment_authors))]\n",
    "            row['cmnt_author']= comment_authors\n",
    "\n",
    "            created = [pd.to_datetime(str(i.created),unit='s') for i in top_levels]\n",
    "            row['cmnt_created'] = created\n",
    "            row['cmnt_all_awardings'] = [i.all_awardings for i in top_levels] \n",
    "            row['cmnt_controversiality'] = [i.controversiality for i in top_levels]\n",
    "            row['cmnt_downs'] = [i.downs for i in top_levels]\n",
    "            row['cmnt_gilded'] = [i.gilded for i in top_levels]\n",
    "            row['cmnt_gildings'] = [i.gildings for i in top_levels]\n",
    "            row['cmnt_likes'] = [i.likes for i in top_levels]\n",
    "            row['cmnt_num_reports'] = [i.num_reports for i in top_levels]\n",
    "            row['cmnt_score'] = [i.score for i in top_levels]\n",
    "            row['cmnt_top_awarded_type'] = [i.top_awarded_type for i in top_levels]\n",
    "            row['cmnt_total_awards_received'] = [i.total_awards_received for i in top_levels]\n",
    "            row['cmnt_ups'] = [i.ups for i in top_levels]\n",
    "\n",
    "\n",
    "            rpl = [] \n",
    "            rpl_author = []\n",
    "            rpl_created = []\n",
    "            rpl_all_awardings = []\n",
    "            rpl_controversiality = []\n",
    "            rpl_downs = []\n",
    "            rpl_gilded = []\n",
    "            rpl_gildings = []\n",
    "            rpl_likes = []\n",
    "            rpl_num_reports = []\n",
    "            rpl_score = []\n",
    "            rpl_top_awarded_type = []\n",
    "            rpl_total_awards_received = []\n",
    "            rpl_ups = []\n",
    "            for comment in top_levels:\n",
    "                rpl.append([i.body for i in comment.replies])\n",
    "                rpl_author.append([i.author for i in comment.replies])\n",
    "                rpl_author[-1] = ['' if rpl_author[-1][i] is None else rpl_author[-1][i].name for i in range(len(rpl_author[-1]))]\n",
    "                rpl_created.append([pd.to_datetime(str(i.created),unit='s') for i in comment.replies])\n",
    "                rpl_all_awardings.append([i.all_awardings for i in comment.replies])\n",
    "                rpl_controversiality.append([i.controversiality for i in comment.replies])\n",
    "                rpl_downs.append([i.downs for i in comment.replies])\n",
    "                rpl_gilded.append([i.gilded for i in comment.replies])\n",
    "                rpl_gildings.append([i.gildings for i in comment.replies])\n",
    "                rpl_likes.append([i.likes for i in comment.replies])\n",
    "                rpl_num_reports.append([i.num_reports for i in comment.replies])\n",
    "                rpl_score.append([i.score for i in comment.replies])\n",
    "                rpl_top_awarded_type.append([i.top_awarded_type for i in comment.replies])\n",
    "                rpl_total_awards_received.append([i.total_awards_received for i in comment.replies])\n",
    "                rpl_ups.append([i.ups for i in comment.replies])\n",
    "\n",
    "            row['replies'] = rpl\n",
    "            row['rpl_author'] = rpl_author\n",
    "            row['rpl_created'] = rpl_created\n",
    "            row['rpl_all_awardings'] = rpl_all_awardings\n",
    "            row['rpl_controversiality'] = rpl_controversiality\n",
    "            row['rpl_downs'] = rpl_downs\n",
    "            row['rpl_gilded'] = rpl_gilded\n",
    "            row['rpl_gildings'] = rpl_gildings\n",
    "            row['rpl_likes'] = rpl_likes\n",
    "            row['rpl_num_reports'] = rpl_num_reports\n",
    "            row['rpl_score'] = rpl_score\n",
    "            row['rpl_top_awarded_type'] = rpl_top_awarded_type\n",
    "            row['rpl_total_awards_received'] = rpl_total_awards_received\n",
    "            row['rpl_ups'] = rpl_ups\n",
    "\n",
    "            top_df = top_df.append(row, ignore_index=True)\n",
    "            \n",
    "    return top_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_new = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "dt = datetime.today().strftime('%Y%m%d') \n",
    "filename = 'Discussion_top_all_100_20200610.json'\n",
    "\n",
    "    \n",
    "sub_title = 'Discussion'\n",
    "sort_type = 'top'\n",
    "time = 'all'\n",
    "limit = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not fetch_new:\n",
    "    df = pd.read_json(filename)\n",
    "else:\n",
    "    reddit = create_reddit_object()\n",
    "    subreddit = reddit.subreddit(sub_title)\n",
    "\n",
    "    df = get_subreddit_data(subreddit, sort_type, time='all', limit=limit)\n",
    "    filename = sub_title + '_' + sort_type + '_' + time + '_' + str(limit) + '_' + dt + '.json'\n",
    "    df.to_json(filename,date_format='iso', date_unit='ns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   2020-04-24 12:59:36\n",
       "1   2020-03-29 22:55:14\n",
       "2   2020-06-02 01:13:29\n",
       "3   2020-05-01 18:34:54\n",
       "4   2019-06-18 11:03:00\n",
       "Name: created, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)['created']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_json(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2020-04-24T12:59:36.000000000Z\n",
       "1    2020-03-29T22:55:14.000000000Z\n",
       "2    2020-06-02T01:13:29.000000000Z\n",
       "3    2020-05-01T18:34:54.000000000Z\n",
       "4    2019-06-18T11:03:00.000000000Z\n",
       "Name: created, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head(5)['created']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#.hot\n",
    "#.new\n",
    "#.controversial\n",
    "#.top\n",
    "#.gilded"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# hot = subreddit.hot(limit=10)\n",
    "# new = subreddit.new(limit=10)\n",
    "# controversial = subreddit.controversial(limit=10)\n",
    "# top = subreddit.top(\"all\",limit=10)\n",
    "# gilded = subreddit.gilded(limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('wordnet') # NEED TO INSTALL FOR LEMMATIZATION\n",
    "# nltk.download('averaged_perceptron_tagger') # NEED TO INSTALL FOR POS ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import pos_tag, pos_tag_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def is_noun(tag):\n",
    "    return tag in ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "\n",
    "def is_verb(tag):\n",
    "    return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "def is_adverb(tag):\n",
    "    return tag in ['RB', 'RBR', 'RBS']\n",
    "\n",
    "def is_adjective(tag):\n",
    "    return tag in ['JJ', 'JJR', 'JJS']\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    if is_adjective(tag):\n",
    "        return wn.ADJ\n",
    "    elif is_adverb(tag):\n",
    "        return wn.ADV\n",
    "    elif is_verb(tag):\n",
    "        return wn.VERB\n",
    "    else:\n",
    "        return wn.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_word_toks = []\n",
    "lem = WordNetLemmatizer()\n",
    "# ps = PorterStemmer()\n",
    "\n",
    "for com in df['comments'].values[0]:\n",
    "    sent_toks = sent_tokenize(com)\n",
    "    for sent_tok in sent_toks:\n",
    "        word_toks = [k for k in filter(lambda x: x not in stop_words, word_tokenize(sent_tok))]\n",
    "        word_toks = [tok for tok in word_toks if tok.isalpha()]\n",
    "        sent_word_toks.append(word_toks)\n",
    "\n",
    "snt_wrd_pos_toks = pos_tag_sents(sent_word_toks)\n",
    "snt_wrd_lem_toks = []\n",
    "for i, tok_sent in enumerate(snt_wrd_pos_toks):\n",
    "    tok_sent = [lem.lemmatize(word, penn_to_wn(pos)) for word,pos in tok_sent]\n",
    "    snt_wrd_lem_toks.append(tok_sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63    1587212906000\n",
       "50    1582077826000\n",
       "77    1586487123000\n",
       "23    1590966103000\n",
       "10    1588482355000\n",
       "Name: created, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)['created']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Yeah', ',', 'prank', 'culture', 'sucks', '.'],\n",
       " ['Yeah', 'agree', '.'],\n",
       " ['I',\n",
       "  'always',\n",
       "  'see',\n",
       "  'couples',\n",
       "  'break',\n",
       "  'pranks',\n",
       "  'cheating',\n",
       "  'pranks',\n",
       "  'every',\n",
       "  'often',\n",
       "  'tell',\n",
       "  'starts',\n",
       "  'affect',\n",
       "  'trust',\n",
       "  'real',\n",
       "  'life',\n",
       "  '.'],\n",
       " ['I',\n",
       "  'get',\n",
       "  'funny',\n",
       "  'play',\n",
       "  'jokes',\n",
       "  'eachother',\n",
       "  'making',\n",
       "  'hobby',\n",
       "  'little',\n",
       "  'immature',\n",
       "  'start',\n",
       "  'cause',\n",
       "  'real',\n",
       "  'issues',\n",
       "  'relationship',\n",
       "  '.'],\n",
       " ['Also',\n",
       "  ',',\n",
       "  'feel',\n",
       "  'like',\n",
       "  'couples',\n",
       "  '’',\n",
       "  'reactions',\n",
       "  'obviously',\n",
       "  'fake',\n",
       "  'expect',\n",
       "  'pranked',\n",
       "  'every',\n",
       "  'week',\n",
       "  'keep',\n",
       "  'going',\n",
       "  'along',\n",
       "  'good',\n",
       "  'content',\n",
       "  '.'],\n",
       " ['yeah', '’', 'entertaining', 'feel', 'bad', '😭😂', '(', 'RIP', 'BFvsGF', ')'],\n",
       " ['iTs', 'jUst', 'pRank', 'brO'],\n",
       " ['It',\n",
       "  'seems',\n",
       "  'like',\n",
       "  \"n't\",\n",
       "  'take',\n",
       "  'seriously',\n",
       "  'shows',\n",
       "  'others',\n",
       "  '...',\n",
       "  '.'],\n",
       " ['You', '’', '100', '%', 'right', ',', 'dude', '.'],\n",
       " ['Yah', 'also', '’', 'couple', 'Instagram', '...'],\n",
       " ['I',\n",
       "  'forgot',\n",
       "  'name',\n",
       "  'anyone',\n",
       "  'knows',\n",
       "  'I',\n",
       "  '’',\n",
       "  'talking',\n",
       "  'please',\n",
       "  'verify',\n",
       "  'want',\n",
       "  '.'],\n",
       " ['I',\n",
       "  'see',\n",
       "  'two',\n",
       "  'always',\n",
       "  'yelling',\n",
       "  'specifically',\n",
       "  'female',\n",
       "  'man',\n",
       "  'acting',\n",
       "  'like',\n",
       "  '’',\n",
       "  'talking',\n",
       "  'girls',\n",
       "  '.'],\n",
       " ['It',\n",
       "  '’',\n",
       "  'funny',\n",
       "  'awhile',\n",
       "  '’',\n",
       "  'reoccurring',\n",
       "  'theme',\n",
       "  'I',\n",
       "  '...',\n",
       "  'idk'],\n",
       " ['My',\n",
       "  'best',\n",
       "  'friend',\n",
       "  'growing',\n",
       "  'liked',\n",
       "  'pranks',\n",
       "  'joked',\n",
       "  'lot',\n",
       "  'like',\n",
       "  'sometimes',\n",
       "  'mean',\n",
       "  'things',\n",
       "  'hurtful',\n",
       "  'embarrassing',\n",
       "  'would',\n",
       "  'laugh',\n",
       "  'I',\n",
       "  'believed',\n",
       "  '.'],\n",
       " ['It',\n",
       "  'got',\n",
       "  'point',\n",
       "  'would',\n",
       "  'try',\n",
       "  'tell',\n",
       "  'story',\n",
       "  'something',\n",
       "  'I',\n",
       "  'literally',\n",
       "  'would',\n",
       "  'make',\n",
       "  'believe',\n",
       "  '.'],\n",
       " ['Not',\n",
       "  'sure',\n",
       "  '’',\n",
       "  'totally',\n",
       "  'related',\n",
       "  'couples',\n",
       "  'pranking',\n",
       "  'I',\n",
       "  'know',\n",
       "  'exactly',\n",
       "  '’',\n",
       "  'talking',\n",
       "  '’',\n",
       "  'terrible',\n",
       "  ',',\n",
       "  'I',\n",
       "  'could',\n",
       "  'never',\n",
       "  'someone',\n",
       "  'things',\n",
       "  'like',\n",
       "  '.'],\n",
       " ['I',\n",
       "  'think',\n",
       "  'definitely',\n",
       "  'good',\n",
       "  'example',\n",
       "  'healthy',\n",
       "  'relationship',\n",
       "  '.'],\n",
       " ['The',\n",
       "  'worst',\n",
       "  'I',\n",
       "  'could',\n",
       "  'throw',\n",
       "  'cup',\n",
       "  'cold',\n",
       "  'water',\n",
       "  'shower',\n",
       "  'I',\n",
       "  '’',\n",
       "  'still',\n",
       "  'feel',\n",
       "  'bad',\n",
       "  '.'],\n",
       " ['All',\n",
       "  'stuff',\n",
       "  'like',\n",
       "  'weird',\n",
       "  'kid',\n",
       "  'channels',\n",
       "  'shit',\n",
       "  'odd',\n",
       "  'toxic',\n",
       "  'shit',\n",
       "  'find',\n",
       "  ',',\n",
       "  'used',\n",
       "  'generate',\n",
       "  'views'],\n",
       " ['I', 'really', 'hate', 'prank', 'channels', '.'],\n",
       " ['Also', ',', 'I', 'hate', 'family', 'prank', 'channels', 'even', '.'],\n",
       " ['When',\n",
       "  'kid',\n",
       "  'actually',\n",
       "  'crying',\n",
       "  '’',\n",
       "  'saying',\n",
       "  '“',\n",
       "  '’',\n",
       "  'prank',\n",
       "  '!',\n",
       "  '”',\n",
       "  'gets',\n",
       "  'serious']]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_word_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Yeah, prank culture sucks.'],\n",
       " ['Yeah i agree.',\n",
       "  'I always see couples doing break up pranks or cheating pranks on each other every so often and you can tell it starts to affect their trust in real life.',\n",
       "  'I get it can be funny to play jokes on eachother here and there but making a hobby out of it can be a little immature and start to cause real issues in the relationship.',\n",
       "  'Also, i feel like after a while the couples’ reactions are obviously fake as they expect to be pranked every week and just keep going along with it for good content.',\n",
       "  'yeah they’re entertaining but i feel bad for them 😭😂(RIP BFvsGF)'],\n",
       " ['iTs jUst a pRank brO'],\n",
       " [\"It just seems like they don't take each other seriously and shows others to do the same....\"],\n",
       " ['You’re 100% right, my dude.'],\n",
       " ['Yah also there’s this on couple on Instagram ...',\n",
       "  'I forgot the name but if anyone knows who I’m talking about please verify it if you want.',\n",
       "  'I just see that these two are always yelling at each other specifically the female while her man is acting like he’s talking to other girls.',\n",
       "  'It’s funny for awhile but if it’s a reoccurring theme I just... idk'],\n",
       " ['My best friend growing up liked to do pranks or just joked a lot but like sometimes mean things that were hurtful or embarrassing and would laugh at me when I believed her.',\n",
       "  'It got to the point where she would try to tell me a story or something and I literally would make myself not believe her.',\n",
       "  'Not sure if that’s totally related to the couples pranking each other but I know exactly what you’re talking about and it’s terrible, I could never be with someone that did things like that.',\n",
       "  'I think those are definitely not a good example of a healthy relationship.',\n",
       "  'The worst I could do is throw a cup of cold water into the shower and I’d still feel bad.'],\n",
       " ['All that stuff is the same like the weird kid channels and shit there is some odd toxic shit you can find, used to generate views'],\n",
       " ['I really hate prank channels.',\n",
       "  'Also, I hate family prank channels even more.',\n",
       "  'When your kid is actually crying and you’re saying “it’s just a prank!” it gets more serious']]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

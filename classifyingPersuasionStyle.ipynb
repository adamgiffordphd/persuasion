{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import json"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "client_id = 'client id code'\n",
    "client_secret = 'client secret'\n",
    "user_agent = 'testPersuation'\n",
    "username = 'dagifft219'\n",
    "password = 'password'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "reddit = praw.Reddit(client_id=client_id,client_secret=client_secret,\n",
    "                    user_agent=user_agent,username=username,password=password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reddit_object(json_file='reddit_config.json',json_key='reddit_user_values'):\n",
    "    with open(json_file) as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    user_values = data[json_key]\n",
    "    \n",
    "    reddit = praw.Reddit(client_id=user_values['client_id'],\n",
    "                         client_secret=user_values['client_secret'],\n",
    "                         user_agent=user_values['user_agent'],\n",
    "                         username=user_values['username'],\n",
    "                         password=user_values['password'])\n",
    "    \n",
    "    return reddit\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subreddit_data(subreddit, sort_type, time='all', limit=10):\n",
    "#     top = subreddit.top(\"all\",limit=100)\n",
    "    attr = getattr(subreddit, sort_type)\n",
    "    if sort_type=='top':\n",
    "        data = attr(time, limit=limit)\n",
    "    else:\n",
    "        data = attr(limit=limit)\n",
    "        \n",
    "    top_df = pd.DataFrame()\n",
    "    for submission in data:\n",
    "        if submission.stickied:\n",
    "            pass\n",
    "        else:\n",
    "            submission.comments.replace_more(limit=None)\n",
    "            row = {}\n",
    "            row['id']= submission.id\n",
    "            row['title']= submission.title\n",
    "            row['url']= submission.url\n",
    "            row['author']= ['' if submission.author is None else submission.author.name][0]\n",
    "            row['created']= pd.to_datetime(str(submission.created),unit='s')\n",
    "            row['submission_text']= submission.selftext\n",
    "            row['all_awardings']= submission.all_awardings\n",
    "            row['category']= submission.category\n",
    "            row['discussion_type']= submission.discussion_type\n",
    "            row['distinguished']= submission.distinguished\n",
    "            row['downs']= submission.downs\n",
    "            row['gilded']= submission.gilded\n",
    "            row['gildings']= submission.gildings\n",
    "            row['likes']= submission.likes\n",
    "            row['num_comments']= submission.num_comments\n",
    "            row['num_crossposts']= submission.num_crossposts\n",
    "            row['num_reports']= submission.num_reports\n",
    "            row['score']= submission.score\n",
    "            row['top_awarded_type']= submission.top_awarded_type\n",
    "            row['total_awards_received']= submission.total_awards_received\n",
    "            row['ups']= submission.ups\n",
    "            row['upvote_ratio']= submission.upvote_ratio\n",
    "            row['view_count']= submission.view_count\n",
    "\n",
    "            top_levels = submission.comments[:]\n",
    "            comments = [i.body for i in top_levels]\n",
    "            row['comments']= comments\n",
    "\n",
    "            comment_authors = [i.author for i in top_levels]\n",
    "            comment_authors = ['' if comment_authors[i] is None else comment_authors[i].name for i in range(len(comment_authors))]\n",
    "            row['cmnt_author']= comment_authors\n",
    "\n",
    "            created = [pd.to_datetime(str(i.created),unit='s') for i in top_levels]\n",
    "            row['cmnt_created'] = created\n",
    "            row['cmnt_all_awardings'] = [i.all_awardings for i in top_levels] \n",
    "            row['cmnt_controversiality'] = [i.controversiality for i in top_levels]\n",
    "            row['cmnt_downs'] = [i.downs for i in top_levels]\n",
    "            row['cmnt_gilded'] = [i.gilded for i in top_levels]\n",
    "            row['cmnt_gildings'] = [i.gildings for i in top_levels]\n",
    "            row['cmnt_likes'] = [i.likes for i in top_levels]\n",
    "            row['cmnt_num_reports'] = [i.num_reports for i in top_levels]\n",
    "            row['cmnt_score'] = [i.score for i in top_levels]\n",
    "            row['cmnt_top_awarded_type'] = [i.top_awarded_type for i in top_levels]\n",
    "            row['cmnt_total_awards_received'] = [i.total_awards_received for i in top_levels]\n",
    "            row['cmnt_ups'] = [i.ups for i in top_levels]\n",
    "\n",
    "\n",
    "            rpl = [] \n",
    "            rpl_author = []\n",
    "            rpl_created = []\n",
    "            rpl_all_awardings = []\n",
    "            rpl_controversiality = []\n",
    "            rpl_downs = []\n",
    "            rpl_gilded = []\n",
    "            rpl_gildings = []\n",
    "            rpl_likes = []\n",
    "            rpl_num_reports = []\n",
    "            rpl_score = []\n",
    "            rpl_top_awarded_type = []\n",
    "            rpl_total_awards_received = []\n",
    "            rpl_ups = []\n",
    "            for comment in top_levels:\n",
    "                rpl.append([i.body for i in comment.replies])\n",
    "                rpl_author.append([i.author for i in comment.replies])\n",
    "                rpl_author[-1] = ['' if rpl_author[-1][i] is None else rpl_author[-1][i].name for i in range(len(rpl_author[-1]))]\n",
    "                rpl_created.append([pd.to_datetime(str(i.created),unit='s') for i in comment.replies])\n",
    "                rpl_all_awardings.append([i.all_awardings for i in comment.replies])\n",
    "                rpl_controversiality.append([i.controversiality for i in comment.replies])\n",
    "                rpl_downs.append([i.downs for i in comment.replies])\n",
    "                rpl_gilded.append([i.gilded for i in comment.replies])\n",
    "                rpl_gildings.append([i.gildings for i in comment.replies])\n",
    "                rpl_likes.append([i.likes for i in comment.replies])\n",
    "                rpl_num_reports.append([i.num_reports for i in comment.replies])\n",
    "                rpl_score.append([i.score for i in comment.replies])\n",
    "                rpl_top_awarded_type.append([i.top_awarded_type for i in comment.replies])\n",
    "                rpl_total_awards_received.append([i.total_awards_received for i in comment.replies])\n",
    "                rpl_ups.append([i.ups for i in comment.replies])\n",
    "\n",
    "            row['replies'] = rpl\n",
    "            row['rpl_author'] = rpl_author\n",
    "            row['rpl_created'] = rpl_created\n",
    "            row['rpl_all_awardings'] = rpl_all_awardings\n",
    "            row['rpl_controversiality'] = rpl_controversiality\n",
    "            row['rpl_downs'] = rpl_downs\n",
    "            row['rpl_gilded'] = rpl_gilded\n",
    "            row['rpl_gildings'] = rpl_gildings\n",
    "            row['rpl_likes'] = rpl_likes\n",
    "            row['rpl_num_reports'] = rpl_num_reports\n",
    "            row['rpl_score'] = rpl_score\n",
    "            row['rpl_top_awarded_type'] = rpl_top_awarded_type\n",
    "            row['rpl_total_awards_received'] = rpl_total_awards_received\n",
    "            row['rpl_ups'] = rpl_ups\n",
    "\n",
    "            top_df = top_df.append(row, ignore_index=True)\n",
    "            \n",
    "    return top_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_new = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "dt = datetime.today().strftime('%Y%m%d') \n",
    "filename = 'Discussion_top_all_100_20200604.json'\n",
    "\n",
    "    \n",
    "sub_title = 'Discussion'\n",
    "sort_type = 'top'\n",
    "time = 'all'\n",
    "limit = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not fetch_new:\n",
    "    df = pd.read_json(filename)\n",
    "else:\n",
    "    reddit = create_reddit_object()\n",
    "    subreddit = reddit.subreddit(sub_title)\n",
    "\n",
    "    df = get_subreddit_data(subreddit, sort_type, time='all', limit=limit)\n",
    "    df.to_json(sub_title + '_' + sort_type + '_' + time + '_' + str(limit) + '_' + dt + '.json')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#.hot\n",
    "#.new\n",
    "#.controversial\n",
    "#.top\n",
    "#.gilded"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# hot = subreddit.hot(limit=10)\n",
    "# new = subreddit.new(limit=10)\n",
    "# controversial = subreddit.controversial(limit=10)\n",
    "# top = subreddit.top(\"all\",limit=10)\n",
    "# gilded = subreddit.gilded(limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('wordnet') # NEED TO INSTALL FOR LEMMATIZATION\n",
    "# nltk.download('averaged_perceptron_tagger') # NEED TO INSTALL FOR POS ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import pos_tag, pos_tag_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def is_noun(tag):\n",
    "    return tag in ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "\n",
    "def is_verb(tag):\n",
    "    return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "def is_adverb(tag):\n",
    "    return tag in ['RB', 'RBR', 'RBS']\n",
    "\n",
    "def is_adjective(tag):\n",
    "    return tag in ['JJ', 'JJR', 'JJS']\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    if is_adjective(tag):\n",
    "        return wn.ADJ\n",
    "    elif is_adverb(tag):\n",
    "        return wn.ADV\n",
    "    elif is_verb(tag):\n",
    "        return wn.VERB\n",
    "    else:\n",
    "        return wn.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_word_toks = []\n",
    "lem = WordNetLemmatizer()\n",
    "# ps = PorterStemmer()\n",
    "\n",
    "for com in df['comments'].values[0]:\n",
    "    sent_toks = sent_tokenize(com)\n",
    "    for sent_tok in sent_toks:\n",
    "        word_toks = [k for k in filter(lambda x: x not in stop_words, word_tokenize(sent_tok))]\n",
    "        sent_word_toks.append(word_toks)\n",
    "\n",
    "snt_wrd_pos_toks = pos_tag_sents(sent_word_toks)\n",
    "snt_wrd_lem_toks = []\n",
    "for i, tok_sent in enumerate(snt_wrd_pos_toks):\n",
    "    tok_sent = [lem.lemmatize(word, penn_to_wn(pos)) for word,pos in tok_sent]\n",
    "    snt_wrd_lem_toks.append(tok_sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Yeah', ',', 'prank', 'culture', 'suck', '.'],\n",
       " ['Yeah', 'agree', '.'],\n",
       " ['I',\n",
       "  'always',\n",
       "  'see',\n",
       "  'couple',\n",
       "  'break',\n",
       "  'prank',\n",
       "  'cheat',\n",
       "  'prank',\n",
       "  'every',\n",
       "  'often',\n",
       "  'tell',\n",
       "  'start',\n",
       "  'affect',\n",
       "  'trust',\n",
       "  'real',\n",
       "  'life',\n",
       "  '.'],\n",
       " ['I',\n",
       "  'get',\n",
       "  'funny',\n",
       "  'play',\n",
       "  'joke',\n",
       "  'eachother',\n",
       "  'make',\n",
       "  'hobby',\n",
       "  'little',\n",
       "  'immature',\n",
       "  'start',\n",
       "  'cause',\n",
       "  'real',\n",
       "  'issue',\n",
       "  'relationship',\n",
       "  '.'],\n",
       " ['Also',\n",
       "  ',',\n",
       "  'feel',\n",
       "  'like',\n",
       "  'couple',\n",
       "  '‚Äô',\n",
       "  'reaction',\n",
       "  'obviously',\n",
       "  'fake',\n",
       "  'expect',\n",
       "  'prank',\n",
       "  'every',\n",
       "  'week',\n",
       "  'keep',\n",
       "  'go',\n",
       "  'along',\n",
       "  'good',\n",
       "  'content',\n",
       "  '.'],\n",
       " ['yeah', '‚Äô', 'entertain', 'feel', 'bad', 'üò≠üòÇ', '(', 'RIP', 'BFvsGF', ')'],\n",
       " ['iTs', 'jUst', 'pRank', 'brO'],\n",
       " ['It',\n",
       "  'seem',\n",
       "  'like',\n",
       "  \"n't\",\n",
       "  'take',\n",
       "  'seriously',\n",
       "  'show',\n",
       "  'others',\n",
       "  '...',\n",
       "  '.'],\n",
       " ['You', '‚Äô', '100', '%', 'right', ',', 'dude', '.'],\n",
       " ['Yah', 'also', '‚Äô', 'couple', 'Instagram', '...'],\n",
       " ['I',\n",
       "  'forget',\n",
       "  'name',\n",
       "  'anyone',\n",
       "  'know',\n",
       "  'I',\n",
       "  '‚Äô',\n",
       "  'talk',\n",
       "  'please',\n",
       "  'verify',\n",
       "  'want',\n",
       "  '.'],\n",
       " ['I',\n",
       "  'see',\n",
       "  'two',\n",
       "  'always',\n",
       "  'yell',\n",
       "  'specifically',\n",
       "  'female',\n",
       "  'man',\n",
       "  'act',\n",
       "  'like',\n",
       "  '‚Äô',\n",
       "  'talk',\n",
       "  'girl',\n",
       "  '.'],\n",
       " ['It',\n",
       "  '‚Äô',\n",
       "  'funny',\n",
       "  'awhile',\n",
       "  '‚Äô',\n",
       "  'reoccurring',\n",
       "  'theme',\n",
       "  'I',\n",
       "  '...',\n",
       "  'idk'],\n",
       " ['My',\n",
       "  'best',\n",
       "  'friend',\n",
       "  'grow',\n",
       "  'liked',\n",
       "  'prank',\n",
       "  'joke',\n",
       "  'lot',\n",
       "  'like',\n",
       "  'sometimes',\n",
       "  'mean',\n",
       "  'thing',\n",
       "  'hurtful',\n",
       "  'embarrass',\n",
       "  'would',\n",
       "  'laugh',\n",
       "  'I',\n",
       "  'believe',\n",
       "  '.'],\n",
       " ['It',\n",
       "  'get',\n",
       "  'point',\n",
       "  'would',\n",
       "  'try',\n",
       "  'tell',\n",
       "  'story',\n",
       "  'something',\n",
       "  'I',\n",
       "  'literally',\n",
       "  'would',\n",
       "  'make',\n",
       "  'believe',\n",
       "  '.'],\n",
       " ['Not',\n",
       "  'sure',\n",
       "  '‚Äô',\n",
       "  'totally',\n",
       "  'related',\n",
       "  'couple',\n",
       "  'prank',\n",
       "  'I',\n",
       "  'know',\n",
       "  'exactly',\n",
       "  '‚Äô',\n",
       "  'talk',\n",
       "  '‚Äô',\n",
       "  'terrible',\n",
       "  ',',\n",
       "  'I',\n",
       "  'could',\n",
       "  'never',\n",
       "  'someone',\n",
       "  'thing',\n",
       "  'like',\n",
       "  '.'],\n",
       " ['I',\n",
       "  'think',\n",
       "  'definitely',\n",
       "  'good',\n",
       "  'example',\n",
       "  'healthy',\n",
       "  'relationship',\n",
       "  '.'],\n",
       " ['The',\n",
       "  'bad',\n",
       "  'I',\n",
       "  'could',\n",
       "  'throw',\n",
       "  'cup',\n",
       "  'cold',\n",
       "  'water',\n",
       "  'shower',\n",
       "  'I',\n",
       "  '‚Äô',\n",
       "  'still',\n",
       "  'feel',\n",
       "  'bad',\n",
       "  '.'],\n",
       " ['All',\n",
       "  'stuff',\n",
       "  'like',\n",
       "  'weird',\n",
       "  'kid',\n",
       "  'channel',\n",
       "  'shit',\n",
       "  'odd',\n",
       "  'toxic',\n",
       "  'shit',\n",
       "  'find',\n",
       "  ',',\n",
       "  'use',\n",
       "  'generate',\n",
       "  'view'],\n",
       " ['I', 'really', 'hate', 'prank', 'channel', '.'],\n",
       " ['Also', ',', 'I', 'hate', 'family', 'prank', 'channel', 'even', '.'],\n",
       " ['When',\n",
       "  'kid',\n",
       "  'actually',\n",
       "  'cry',\n",
       "  '‚Äô',\n",
       "  'say',\n",
       "  '‚Äú',\n",
       "  '‚Äô',\n",
       "  'prank',\n",
       "  '!',\n",
       "  '‚Äù',\n",
       "  'get',\n",
       "  'serious']]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snt_wrd_lem_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Yeah', ',', 'prank', 'culture', 'sucks', '.'],\n",
       " ['Yeah', 'agree', '.'],\n",
       " ['I',\n",
       "  'always',\n",
       "  'see',\n",
       "  'couples',\n",
       "  'break',\n",
       "  'pranks',\n",
       "  'cheating',\n",
       "  'pranks',\n",
       "  'every',\n",
       "  'often',\n",
       "  'tell',\n",
       "  'starts',\n",
       "  'affect',\n",
       "  'trust',\n",
       "  'real',\n",
       "  'life',\n",
       "  '.'],\n",
       " ['I',\n",
       "  'get',\n",
       "  'funny',\n",
       "  'play',\n",
       "  'jokes',\n",
       "  'eachother',\n",
       "  'making',\n",
       "  'hobby',\n",
       "  'little',\n",
       "  'immature',\n",
       "  'start',\n",
       "  'cause',\n",
       "  'real',\n",
       "  'issues',\n",
       "  'relationship',\n",
       "  '.'],\n",
       " ['Also',\n",
       "  ',',\n",
       "  'feel',\n",
       "  'like',\n",
       "  'couples',\n",
       "  '‚Äô',\n",
       "  'reactions',\n",
       "  'obviously',\n",
       "  'fake',\n",
       "  'expect',\n",
       "  'pranked',\n",
       "  'every',\n",
       "  'week',\n",
       "  'keep',\n",
       "  'going',\n",
       "  'along',\n",
       "  'good',\n",
       "  'content',\n",
       "  '.'],\n",
       " ['yeah', '‚Äô', 'entertaining', 'feel', 'bad', 'üò≠üòÇ', '(', 'RIP', 'BFvsGF', ')'],\n",
       " ['iTs', 'jUst', 'pRank', 'brO'],\n",
       " ['It',\n",
       "  'seems',\n",
       "  'like',\n",
       "  \"n't\",\n",
       "  'take',\n",
       "  'seriously',\n",
       "  'shows',\n",
       "  'others',\n",
       "  '...',\n",
       "  '.'],\n",
       " ['You', '‚Äô', '100', '%', 'right', ',', 'dude', '.'],\n",
       " ['Yah', 'also', '‚Äô', 'couple', 'Instagram', '...'],\n",
       " ['I',\n",
       "  'forgot',\n",
       "  'name',\n",
       "  'anyone',\n",
       "  'knows',\n",
       "  'I',\n",
       "  '‚Äô',\n",
       "  'talking',\n",
       "  'please',\n",
       "  'verify',\n",
       "  'want',\n",
       "  '.'],\n",
       " ['I',\n",
       "  'see',\n",
       "  'two',\n",
       "  'always',\n",
       "  'yelling',\n",
       "  'specifically',\n",
       "  'female',\n",
       "  'man',\n",
       "  'acting',\n",
       "  'like',\n",
       "  '‚Äô',\n",
       "  'talking',\n",
       "  'girls',\n",
       "  '.'],\n",
       " ['It',\n",
       "  '‚Äô',\n",
       "  'funny',\n",
       "  'awhile',\n",
       "  '‚Äô',\n",
       "  'reoccurring',\n",
       "  'theme',\n",
       "  'I',\n",
       "  '...',\n",
       "  'idk'],\n",
       " ['My',\n",
       "  'best',\n",
       "  'friend',\n",
       "  'growing',\n",
       "  'liked',\n",
       "  'pranks',\n",
       "  'joked',\n",
       "  'lot',\n",
       "  'like',\n",
       "  'sometimes',\n",
       "  'mean',\n",
       "  'things',\n",
       "  'hurtful',\n",
       "  'embarrassing',\n",
       "  'would',\n",
       "  'laugh',\n",
       "  'I',\n",
       "  'believed',\n",
       "  '.'],\n",
       " ['It',\n",
       "  'got',\n",
       "  'point',\n",
       "  'would',\n",
       "  'try',\n",
       "  'tell',\n",
       "  'story',\n",
       "  'something',\n",
       "  'I',\n",
       "  'literally',\n",
       "  'would',\n",
       "  'make',\n",
       "  'believe',\n",
       "  '.'],\n",
       " ['Not',\n",
       "  'sure',\n",
       "  '‚Äô',\n",
       "  'totally',\n",
       "  'related',\n",
       "  'couples',\n",
       "  'pranking',\n",
       "  'I',\n",
       "  'know',\n",
       "  'exactly',\n",
       "  '‚Äô',\n",
       "  'talking',\n",
       "  '‚Äô',\n",
       "  'terrible',\n",
       "  ',',\n",
       "  'I',\n",
       "  'could',\n",
       "  'never',\n",
       "  'someone',\n",
       "  'things',\n",
       "  'like',\n",
       "  '.'],\n",
       " ['I',\n",
       "  'think',\n",
       "  'definitely',\n",
       "  'good',\n",
       "  'example',\n",
       "  'healthy',\n",
       "  'relationship',\n",
       "  '.'],\n",
       " ['The',\n",
       "  'worst',\n",
       "  'I',\n",
       "  'could',\n",
       "  'throw',\n",
       "  'cup',\n",
       "  'cold',\n",
       "  'water',\n",
       "  'shower',\n",
       "  'I',\n",
       "  '‚Äô',\n",
       "  'still',\n",
       "  'feel',\n",
       "  'bad',\n",
       "  '.'],\n",
       " ['All',\n",
       "  'stuff',\n",
       "  'like',\n",
       "  'weird',\n",
       "  'kid',\n",
       "  'channels',\n",
       "  'shit',\n",
       "  'odd',\n",
       "  'toxic',\n",
       "  'shit',\n",
       "  'find',\n",
       "  ',',\n",
       "  'used',\n",
       "  'generate',\n",
       "  'views'],\n",
       " ['I', 'really', 'hate', 'prank', 'channels', '.'],\n",
       " ['Also', ',', 'I', 'hate', 'family', 'prank', 'channels', 'even', '.'],\n",
       " ['When',\n",
       "  'kid',\n",
       "  'actually',\n",
       "  'crying',\n",
       "  '‚Äô',\n",
       "  'saying',\n",
       "  '‚Äú',\n",
       "  '‚Äô',\n",
       "  'prank',\n",
       "  '!',\n",
       "  '‚Äù',\n",
       "  'gets',\n",
       "  'serious']]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_word_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Yeah, prank culture sucks.'],\n",
       " ['Yeah i agree.',\n",
       "  'I always see couples doing break up pranks or cheating pranks on each other every so often and you can tell it starts to affect their trust in real life.',\n",
       "  'I get it can be funny to play jokes on eachother here and there but making a hobby out of it can be a little immature and start to cause real issues in the relationship.',\n",
       "  'Also, i feel like after a while the couples‚Äô reactions are obviously fake as they expect to be pranked every week and just keep going along with it for good content.',\n",
       "  'yeah they‚Äôre entertaining but i feel bad for them üò≠üòÇ(RIP BFvsGF)'],\n",
       " ['iTs jUst a pRank brO'],\n",
       " [\"It just seems like they don't take each other seriously and shows others to do the same....\"],\n",
       " ['You‚Äôre 100% right, my dude.'],\n",
       " ['Yah also there‚Äôs this on couple on Instagram ...',\n",
       "  'I forgot the name but if anyone knows who I‚Äôm talking about please verify it if you want.',\n",
       "  'I just see that these two are always yelling at each other specifically the female while her man is acting like he‚Äôs talking to other girls.',\n",
       "  'It‚Äôs funny for awhile but if it‚Äôs a reoccurring theme I just... idk'],\n",
       " ['My best friend growing up liked to do pranks or just joked a lot but like sometimes mean things that were hurtful or embarrassing and would laugh at me when I believed her.',\n",
       "  'It got to the point where she would try to tell me a story or something and I literally would make myself not believe her.',\n",
       "  'Not sure if that‚Äôs totally related to the couples pranking each other but I know exactly what you‚Äôre talking about and it‚Äôs terrible, I could never be with someone that did things like that.',\n",
       "  'I think those are definitely not a good example of a healthy relationship.',\n",
       "  'The worst I could do is throw a cup of cold water into the shower and I‚Äôd still feel bad.'],\n",
       " ['All that stuff is the same like the weird kid channels and shit there is some odd toxic shit you can find, used to generate views'],\n",
       " ['I really hate prank channels.',\n",
       "  'Also, I hate family prank channels even more.',\n",
       "  'When your kid is actually crying and you‚Äôre saying ‚Äúit‚Äôs just a prank!‚Äù it gets more serious']]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
